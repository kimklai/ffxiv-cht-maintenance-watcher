import requests
import re
import os
import time
from playwright.sync_api import sync_playwright

# --- é…ç½®å€ ---
webhook_raw = os.environ.get("DISCORD_WEBHOOK", "")
DISCORD_WEBHOOK_URLS = [url.strip() for url in webhook_raw.split(",") if url.strip()]

# å®šç¾©è¦æª¢æŸ¥çš„åˆ†é¡èˆ‡ Category 4 çš„é»‘åå–®
CATEGORIES = [3, 4]
CAT4_BLACKLIST = ["3", "46", "65"]  # é€™äº›ç·¨è™Ÿçš„å…¬å‘Šæœƒè¢«ç•¥é

def send_to_discord(title, link, text_content):
    if not DISCORD_WEBHOOK_URLS:
        print("âš ï¸ æœªè¨­å®š Discord Webhook")
        return

    if len(text_content) > 3000:
        text_content = text_content[:3000] + "\n\n...(å…§å®¹éé•·)"

    payload = {
        "username": "FFXIV å…‰ä¹‹å·¥å…·äºº",
        "embeds": [{
            "title": title,
            "url": link,
            "description": text_content,
            "color": 3447003,
            "footer": {"text": f"ç™¼é€æ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}"}
        }]
    }

    for url in DISCORD_WEBHOOK_URLS:
        requests.post(url, json=payload)

def run_scraper():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()

        for cat in CATEGORIES:
            try:
                url = f"https://www.ffxiv.com.tw/web/news/news_list.aspx?category={cat}"
                print(f"æ­£åœ¨æª¢æŸ¥åˆ†é¡ {cat}: {url}")
                page.goto(url, timeout=60000)
                page.wait_for_selector(".news_list .item")

                # æŠ“å–ç¬¬ä¸€å‰‡å…¬å‘Š
                first_item = page.query_selector(".news_list .item")
                news_id = first_item.query_selector(".news_id").inner_text().strip()

                # é‡å° Category 4 çš„é»‘åå–®æª¢æŸ¥
                if cat == 4 and news_id in CAT4_BLACKLIST:
                    print(f"â© ç•¥é Category 4 çš„é»‘åå–®ç·¨è™Ÿ: {news_id}")
                    continue

                title = first_item.query_selector(".title a").inner_text().strip()
                link = "https://www.ffxiv.com.tw" + first_item.query_selector(".title a").get_attribute("href")

                # æª¢æŸ¥æ›´æ–° (æ¯å€‹åˆ†é¡ç¨ç«‹æª”æ¡ˆ)
                record_file = f"last_news_id_{cat}.txt"
                if os.path.exists(record_file):
                    with open(record_file, "r", encoding="utf-8") as f:
                        if f.read().strip() == news_id:
                            print(f"ğŸ˜´ åˆ†é¡ {cat} æ²’æœ‰æ–°å…¬å‘Šã€‚")
                            continue

                # é€²å…¥å…§æ–‡æŠ“å– .article
                page.goto(link, timeout=60000)
                page.wait_for_selector(".article")
                article_text = page.query_selector(".article").inner_text().strip()
                formatted_text = re.sub(r'\n{3,}', '\n\n', article_text)

                # ç™¼é€é€šçŸ¥
                send_to_discord(f"{title}", link, formatted_text)

                # æ›´æ–°ç´€éŒ„ (å­˜ ID æ¯”å­˜æ¨™é¡Œæ›´æº–ç¢º)
                with open(record_file, "w", encoding="utf-8") as f:
                    f.write(news_id)

            except Exception as e:
                print(f"åˆ†é¡ {cat} åŸ·è¡Œå‡ºéŒ¯: {e}")

        browser.close()

if __name__ == "__main__":
    run_scraper()
